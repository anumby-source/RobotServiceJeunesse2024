{
    "LINR1": {
        "id": "LINR1",
        "dirname": "LinearReg",
        "basename": "01-Linear-Regression.ipynb",
        "title": "Linear regression with direct resolution",
        "description": "Low-level implementation, using numpy, of a direct resolution for a linear regression",
        "overrides": []
    },
    "GRAD1": {
        "id": "GRAD1",
        "dirname": "LinearReg",
        "basename": "02-Gradient-descent.ipynb",
        "title": "Linear regression with gradient descent",
        "description": "Low level implementation of a solution by gradient descent. Basic and stochastic approach.",
        "overrides": []
    },
    "POLR1": {
        "id": "POLR1",
        "dirname": "LinearReg",
        "basename": "03-Polynomial-Regression.ipynb",
        "title": "Complexity Syndrome",
        "description": "Illustration of the problem of complexity with the polynomial regression",
        "overrides": []
    },
    "LOGR1": {
        "id": "LOGR1",
        "dirname": "LinearReg",
        "basename": "04-Logistic-Regression.ipynb",
        "title": "Logistic regression",
        "description": "Simple example of logistic regression with a sklearn solution",
        "overrides": []
    },
    "PER57": {
        "id": "PER57",
        "dirname": "IRIS",
        "basename": "01-Simple-Perceptron.ipynb",
        "title": "Perceptron Model 1957",
        "description": "Example of use of a Perceptron, with sklearn and IRIS dataset of 1936 !",
        "overrides": []
    },
    "BHPD1": {
        "id": "BHPD1",
        "dirname": "BHPD",
        "basename": "01-DNN-Regression.ipynb",
        "title": "Regression with a Dense Network (DNN)",
        "description": "Simple example of a regression with the dataset Boston Housing Prices Dataset (BHPD)",
        "overrides": [
            "fit_verbosity"
        ]
    },
    "BHPD2": {
        "id": "BHPD2",
        "dirname": "BHPD",
        "basename": "02-DNN-Regression-Premium.ipynb",
        "title": "Regression with a Dense Network (DNN) - Advanced code",
        "description": "A more advanced implementation of the precedent example",
        "overrides": [
            "fit_verbosity"
        ]
    },
    "MNIST1": {
        "id": "MNIST1",
        "dirname": "MNIST",
        "basename": "01-DNN-MNIST.ipynb",
        "title": "Simple classification with DNN",
        "description": "An example of classification using a dense neural network for the famous MNIST dataset",
        "overrides": [
            "fit_verbosity"
        ]
    },
    "MNIST2": {
        "id": "MNIST2",
        "dirname": "MNIST",
        "basename": "02-CNN-MNIST.ipynb",
        "title": "Simple classification with CNN",
        "description": "An example of classification using a convolutional neural network for the famous MNIST dataset",
        "overrides": [
            "fit_verbosity"
        ]
    },
    "GTSRB1": {
        "id": "GTSRB1",
        "dirname": "GTSRB",
        "basename": "01-Preparation-of-data.ipynb",
        "title": "Dataset analysis and preparation",
        "description": "Episode 1 : Analysis of the GTSRB dataset and creation of an enhanced dataset",
        "overrides": [
            "scale",
            "output_dir",
            "progress_verbosity"
        ]
    },
    "GTSRB2": {
        "id": "GTSRB2",
        "dirname": "GTSRB",
        "basename": "02-First-convolutions.ipynb",
        "title": "First convolutions",
        "description": "Episode 2 : First convolutions and first classification of our traffic signs",
        "overrides": [
            "run_dir",
            "enhanced_dir",
            "dataset_name",
            "batch_size",
            "epochs",
            "scale",
            "fit_verbosity"
        ]
    },
    "GTSRB3": {
        "id": "GTSRB3",
        "dirname": "GTSRB",
        "basename": "03-Tracking-and-visualizing.ipynb",
        "title": "Training monitoring",
        "description": "Episode 3 : Monitoring, analysis and check points during a training session",
        "overrides": [
            "run_dir",
            "enhanced_dir",
            "dataset_name",
            "batch_size",
            "epochs",
            "scale",
            "fit_verbosity"
        ]
    },
    "GTSRB4": {
        "id": "GTSRB4",
        "dirname": "GTSRB",
        "basename": "04-Data-augmentation.ipynb",
        "title": "Data augmentation ",
        "description": "Episode 4 : Adding data by data augmentation when we lack it, to improve our results",
        "overrides": [
            "run_dir",
            "enhanced_dir",
            "dataset_name",
            "batch_size",
            "epochs",
            "scale",
            "fit_verbosity"
        ]
    },
    "GTSRB5": {
        "id": "GTSRB5",
        "dirname": "GTSRB",
        "basename": "05-Full-convolutions.ipynb",
        "title": "Full convolutions",
        "description": "Episode 5 : A lot of models, a lot of datasets and a lot of results.",
        "overrides": [
            "run_dir",
            "enhanced_dir",
            "datasets",
            "models",
            "batch_size",
            "epochs",
            "scale",
            "with_datagen",
            "fit_verbosity"
        ]
    },
    "GTSRB6": {
        "id": "GTSRB6",
        "dirname": "GTSRB",
        "basename": "06-Notebook-as-a-batch.ipynb",
        "title": "Full convolutions as a batch",
        "description": "Episode 6 : To compute bigger, use your notebook in batch mode",
        "overrides": []
    },
    "GTSRB7": {
        "id": "GTSRB7",
        "dirname": "GTSRB",
        "basename": "07-Show-report.ipynb",
        "title": "Batch reports",
        "description": "Episode 7 : Displaying our jobs report, and the winner is...",
        "overrides": [
            "run_dir",
            "report_dir"
        ]
    },
    "GTSRB10": {
        "id": "GTSRB10",
        "dirname": "GTSRB",
        "basename": "batch_oar.sh",
        "title": "OAR batch script submission",
        "description": "Bash script for an OAR batch submission of an ipython code",
        "overrides": []
    },
    "GTSRB11": {
        "id": "GTSRB11",
        "dirname": "GTSRB",
        "basename": "batch_slurm.sh",
        "title": "SLURM batch script",
        "description": "Bash script for a Slurm batch submission of an ipython code",
        "overrides": []
    },
    "IMDB1": {
        "id": "IMDB1",
        "dirname": "IMDB",
        "basename": "01-One-hot-encoding.ipynb",
        "title": "Sentiment analysis with hot-one encoding",
        "description": "A basic example of sentiment analysis with sparse encoding, using a dataset from Internet Movie Database (IMDB)",
        "overrides": [
            "run_dir",
            "vocab_size",
            "hide_most_frequently",
            "batch_size",
            "epochs",
            "fit_verbosity"
        ]
    },
    "IMDB2": {
        "id": "IMDB2",
        "dirname": "IMDB",
        "basename": "02-Keras-embedding.ipynb",
        "title": "Sentiment analysis with text embedding",
        "description": "A very classical example of word embedding with a dataset from Internet Movie Database (IMDB)",
        "overrides": [
            "run_dir",
            "vocab_size",
            "hide_most_frequently",
            "review_len",
            "dense_vector_size",
            "batch_size",
            "epochs",
            "output_dir",
            "fit_verbosity"
        ]
    },
    "IMDB3": {
        "id": "IMDB3",
        "dirname": "IMDB",
        "basename": "03-Prediction.ipynb",
        "title": "Reload and reuse a saved model",
        "description": "Retrieving a saved model to perform a sentiment analysis (movie review)",
        "overrides": [
            "run_dir",
            "vocab_size",
            "review_len",
            "dictionaries_dir"
        ]
    },
    "IMDB4": {
        "id": "IMDB4",
        "dirname": "IMDB",
        "basename": "04-Show-vectors.ipynb",
        "title": "Reload embedded vectors",
        "description": "Retrieving embedded vectors from our trained model",
        "overrides": [
            "run_dir",
            "vocab_size",
            "review_len",
            "dictionaries_dir"
        ]
    },
    "IMDB5": {
        "id": "IMDB5",
        "dirname": "IMDB",
        "basename": "05-LSTM-Keras.ipynb",
        "title": "Sentiment analysis with a RNN network",
        "description": "Still the same problem, but with a network combining embedding and RNN",
        "overrides": [
            "run_dir",
            "vocab_size",
            "hide_most_frequently",
            "review_len",
            "dense_vector_size",
            "batch_size",
            "epochs",
            "fit_verbosity",
            "scale"
        ]
    },
    "LADYB1": {
        "id": "LADYB1",
        "dirname": "SYNOP",
        "basename": "LADYB1-Ladybug.ipynb",
        "title": "Prediction of a 2D trajectory via RNN",
        "description": "Artificial dataset generation and prediction attempt via a recurrent network",
        "overrides": [
            "run_dir",
            "scale",
            "train_prop",
            "sequence_len",
            "predict_len",
            "batch_size",
            "epochs"
        ]
    },
    "SYNOP1": {
        "id": "SYNOP1",
        "dirname": "SYNOP",
        "basename": "SYNOP1-Preparation-of-data.ipynb",
        "title": "Preparation of data",
        "description": "Episode 1 : Data analysis and preparation of a usuable meteorological dataset (SYNOP)",
        "overrides": [
            "run_dir",
            "output_dir"
        ]
    },
    "SYNOP2": {
        "id": "SYNOP2",
        "dirname": "SYNOP",
        "basename": "SYNOP2-First-predictions.ipynb",
        "title": "First predictions at 3h",
        "description": "Episode 2 : RNN training session for weather prediction attempt at 3h",
        "overrides": [
            "run_dir",
            "scale",
            "train_prop",
            "sequence_len",
            "batch_size",
            "epochs",
            "fit_verbosity"
        ]
    },
    "SYNOP3": {
        "id": "SYNOP3",
        "dirname": "SYNOP",
        "basename": "SYNOP3-12h-predictions.ipynb",
        "title": "12h predictions",
        "description": "Episode 3: Attempt to predict in a more longer term ",
        "overrides": [
            "run_dir",
            "iterations",
            "scale",
            "train_prop",
            "sequence_len"
        ]
    },
    "TRANS1": {
        "id": "TRANS1",
        "dirname": "Transformers",
        "basename": "01-Distilbert.ipynb",
        "title": "IMDB, Sentiment analysis with Transformers ",
        "description": "Using a Tranformer to perform a sentiment analysis (IMDB) - Jean Zay version",
        "overrides": []
    },
    "TRANS2": {
        "id": "TRANS2",
        "dirname": "Transformers",
        "basename": "02-distilbert_colab.ipynb",
        "title": "IMDB, Sentiment analysis with Transformers ",
        "description": "Using a Tranformer to perform a sentiment analysis (IMDB) - Colab version",
        "overrides": []
    },
    "AE1": {
        "id": "AE1",
        "dirname": "AE",
        "basename": "01-Prepare-MNIST-dataset.ipynb",
        "title": "Prepare a noisy MNIST dataset",
        "description": "Episode 1: Preparation of a noisy MNIST dataset",
        "overrides": [
            "run_dir",
            "prepared_dataset",
            "scale",
            "progress_verbosity"
        ]
    },
    "AE2": {
        "id": "AE2",
        "dirname": "AE",
        "basename": "02-AE-with-MNIST.ipynb",
        "title": "Building and training an AE denoiser model",
        "description": "Episode 1 : Construction of a denoising autoencoder and training of it with a noisy MNIST dataset.",
        "overrides": [
            "run_dir",
            "prepared_dataset",
            "dataset_seed",
            "scale",
            "latent_dim",
            "train_prop",
            "batch_size",
            "epochs"
        ]
    },
    "AE3": {
        "id": "AE3",
        "dirname": "AE",
        "basename": "03-AE-with-MNIST-post.ipynb",
        "title": "Playing with our denoiser model",
        "description": "Episode 2 : Using the previously trained autoencoder to denoise data",
        "overrides": [
            "run_dir",
            "prepared_dataset",
            "dataset_seed",
            "scale",
            "train_prop"
        ]
    },
    "AE4": {
        "id": "AE4",
        "dirname": "AE",
        "basename": "04-ExtAE-with-MNIST.ipynb",
        "title": "Denoiser and classifier model",
        "description": "Episode 4 : Construction of a denoiser and classifier model",
        "overrides": [
            "run_dir",
            "prepared_dataset",
            "dataset_seed",
            "scale",
            "latent_dim",
            "train_prop",
            "batch_size",
            "epochs"
        ]
    },
    "AE5": {
        "id": "AE5",
        "dirname": "AE",
        "basename": "05-ExtAE-with-MNIST.ipynb",
        "title": "Advanced denoiser and classifier model",
        "description": "Episode 5 : Construction of an advanced denoiser and classifier model",
        "overrides": [
            "run_dir",
            "prepared_dataset",
            "dataset_seed",
            "scale",
            "latent_dim",
            "train_prop",
            "batch_size",
            "epochs"
        ]
    },
    "VAE1": {
        "id": "VAE1",
        "dirname": "VAE",
        "basename": "01-VAE-with-MNIST.ipynb",
        "title": "First VAE, using functional API (MNIST dataset)",
        "description": "Construction and training of a VAE, using functional APPI, with a latent space of small dimension.",
        "overrides": [
            "run_dir",
            "latent_dim",
            "loss_weights",
            "scale",
            "seed",
            "batch_size",
            "epochs",
            "fit_verbosity",
            "run_dir"
        ]
    },
    "VAE2": {
        "id": "VAE2",
        "dirname": "VAE",
        "basename": "02-VAE-with-MNIST.ipynb",
        "title": "VAE, using a custom model class  (MNIST dataset)",
        "description": "Construction and training of a VAE, using model subclass, with a latent space of small dimension.",
        "overrides": [
            "run_dir",
            "latent_dim",
            "loss_weights",
            "scale",
            "seed",
            "batch_size",
            "epochs",
            "fit_verbosity",
            "run_dir"
        ]
    },
    "VAE3": {
        "id": "VAE3",
        "dirname": "VAE",
        "basename": "03-VAE-with-MNIST-post.ipynb",
        "title": "Analysis of the VAE's latent space of MNIST dataset",
        "description": "Visualization and analysis of the VAE's latent space of the dataset MNIST",
        "overrides": [
            "run_dir",
            "scale",
            "seed"
        ]
    },
    "VAE5": {
        "id": "VAE5",
        "dirname": "VAE",
        "basename": "05-About-CelebA.ipynb",
        "title": "Another game play : About the CelebA dataset",
        "description": "Episode 1 : Presentation of the CelebA dataset and problems related to its size",
        "overrides": [
            "run_dir",
            "progress_verbosity"
        ]
    },
    "VAE6": {
        "id": "VAE6",
        "dirname": "VAE",
        "basename": "06-Prepare-CelebA-datasets.ipynb",
        "title": "Generation of a clustered dataset",
        "description": "Episode 2 : Analysis of the CelebA dataset and creation of an clustered and usable dataset",
        "overrides": [
            "run_dir",
            "progress_verbosity",
            "scale",
            "seed",
            "cluster_size",
            "image_size",
            "output_dir",
            "exit_if_exist"
        ]
    },
    "VAE7": {
        "id": "VAE7",
        "dirname": "VAE",
        "basename": "07-Check-CelebA.ipynb",
        "title": "Checking the clustered dataset",
        "description": "Episode : 3 Clustered dataset verification and testing of our datagenerator",
        "overrides": [
            "run_dir",
            "image_size",
            "enhanced_dir",
            "progress_verbosity"
        ]
    },
    "VAE8": {
        "id": "VAE8",
        "dirname": "VAE",
        "basename": "08-VAE-with-CelebA.ipynb",
        "title": "Training session for our VAE",
        "description": "Episode 4 : Training with our clustered datasets in notebook or batch mode",
        "overrides": [
            "run_dir",
            "scale",
            "image_size",
            "enhanced_dir",
            "latent_dim",
            "loss_weights",
            "batch_size",
            "epochs",
            "fit_verbosity",
            "run_dir"
        ]
    },
    "VAE9": {
        "id": "VAE9",
        "dirname": "VAE",
        "basename": "10-VAE-with-CelebA-post.ipynb",
        "title": "Data generation from latent space",
        "description": "Episode 5 : Exploring latent space to generate new data",
        "overrides": [
            "run_dir",
            "image_size",
            "enhanced_dir"
        ]
    },
    "VAE10": {
        "id": "VAE10",
        "dirname": "VAE",
        "basename": "batch_slurm.sh",
        "title": "SLURM batch script",
        "description": "Bash script for SLURM batch submission of VAE8 notebooks ",
        "overrides": []
    },
    "SHEEP1": {
        "id": "SHEEP1",
        "dirname": "DCGAN",
        "basename": "01-DCGAN-Draw-me-a-sheep.ipynb",
        "title": "A first DCGAN to Draw a Sheep",
        "description": "Episode 1 : Draw me a sheep, revisited with a DCGAN",
        "overrides": [
            "run_dir",
            "scale",
            "latent_dim",
            "epochs",
            "batch_size",
            "num_img",
            "fit_verbosity",
            "run_dir"
        ]
    },
    "ACTF1": {
        "id": "ACTF1",
        "dirname": "Misc",
        "basename": "Activation-Functions.ipynb",
        "title": "Activation functions",
        "description": "Some activation functions, with their derivatives.",
        "overrides": []
    },
    "NP1": {
        "id": "NP1",
        "dirname": "Misc",
        "basename": "Numpy.ipynb",
        "title": "A short introduction to Numpy",
        "description": "Numpy is an essential tool for the Scientific Python.",
        "overrides": []
    },
    "SCRATCH1": {
        "id": "SCRATCH1",
        "dirname": "Misc",
        "basename": "Scratchbook.ipynb",
        "title": "Scratchbook",
        "description": "A scratchbook for small examples",
        "overrides": []
    },
    "TSB1": {
        "id": "TSB1",
        "dirname": "Misc",
        "basename": "Using-Tensorboard.ipynb",
        "title": "Tensorboard with/from Jupyter ",
        "description": "4 ways to use Tensorboard from the Jupyter environment",
        "overrides": []
    }
}